{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AML Assignment 2\n",
    "\n",
    "## Supreet Singh - CS16BTECH11038\n",
    "\n",
    "(Done in Python 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) \n",
    "\n",
    "Using Mercer's Condition, \n",
    "\n",
    "$  K(x,z) = K1(x,z) + K2(x,z) $ has to be positive semidefinite, i.e. Integral (dx.dz. f(x).K(x,z).f(z)) >= 0.\n",
    "\n",
    "Since this condition is true for K1 and K2, it is also true for K1+K2 (simply add those two to get the required inequality). Hence, this K is a valid kernal function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)\n",
    "K(x,z) = K1(x,z) . K2(x,z)\n",
    "\n",
    "Let $ K1(x, z) = \\phi_1(x) . \\phi_1(z) $ \n",
    "\n",
    " $ K2(x, z) = \\phi_2(x) . \\phi_2(z) $ \n",
    " \n",
    "Then: (x, z are vectors)\n",
    "\n",
    "$$ K1 . K2 = (\\sum_i \\phi_1(x_i).\\phi_1(z_i)) . (\\sum_j \\phi_2(x_j).\\phi_2(z_j)) $$\n",
    "\n",
    "_note: Here $ \\phi_1(x_i) $ denotes i'th component of $ \\phi_1(x) $ _\n",
    "\n",
    "Which is equal to: \n",
    "\n",
    "$$ K1. K2 = \\sum_{i, j} \\phi_1(x_i).\\phi_2(x_j) . \\phi_1(z_i).\\phi_2(z_j) $$\n",
    "\n",
    "Set a new $$ \\phi_3(x_k) = \\phi_1(x_i).\\phi_2(x_j) $$ (similarly for z).\n",
    "\n",
    "We get, $$ K(x,z) = K1 . K2 = \\sum_k \\phi_3(x_k) . \\phi_3(z_k) = \\phi_3(x) . \\phi_3(z) $$\n",
    "\n",
    " **Hence, this K is decomposable into a dot product, and is a valid Kernel. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c)\n",
    "K(x,z) = h(K1(x,z))\n",
    "\n",
    "Using Mercer's Condition,\n",
    "\n",
    "$$ \\int dx.dz.f(x). K1(x,z) . f(z) \\ge 0 $$\n",
    "\n",
    "for any f(x) satisfying the criteria of Mercer's condition.\n",
    "\n",
    "$$ (\\forall a_i > 0 ): $$  \n",
    "\n",
    "$$ \\implies a_i \\int dx.dz.f(x). K1(x,z) . f(z) \\ge 0 $$\n",
    "\n",
    "\n",
    "\n",
    "$$ \\implies \\int dx.dz.f(x). a_i . K1(x,z) . f(z) \\ge 0 $$\n",
    "\n",
    "$$ \\implies \\int dx.dz.f(x). h(K1(x,z) . f(z) \\ge 0 $$\n",
    "\n",
    "where $$ h(K1) = a_0 + a_1 . K1 + a_2. K1^2 + ... $$ is a polynomial with positive coeffecients  $(a_i > 0 ) $ (Using (a) part for addition of kernel functions resulting in valid kernel function)\n",
    "\n",
    "Thus, ** This is a valid kernel ** because it satisfies mercer's condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d)\n",
    "\n",
    "K(x,z) = $ e^ {K1(x,z)} $\n",
    "\n",
    "$ e^{K1(x,z)} $  can be approximated to a polynomial of K1(x,z) using Taylor Series:\n",
    "\n",
    "$ e^x = 1 + x + \\frac{x^2}{2!} + ... $\n",
    "\n",
    "In part(c), it has been shown that a polynomial function of a valid kernel is also a valid kernel. \n",
    "\n",
    "Thus, **this is also a valid kernel.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e)\n",
    "K(x, z) = exp ($\\frac{-||x - z||^2}{\\sigma^2} $)\n",
    "\n",
    "It is known that K1(x,z) = exp ($\\frac{-||x - z||^2}{2.\\sigma^2} $) is a valid kernel (mentioned in slides).\n",
    "\n",
    "$ K(x,z) = (K1(x,z))^2$.\n",
    "\n",
    "Thus, using part (b), ** this is also a valid kernel. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data!\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "\n",
    "# Open the url for reading\n",
    "f1 = urllib.request.urlopen(\"http://www.amlbook.com/data/zip/features.train\")\n",
    "# fast reading function!\n",
    "rawTrainSet = np.loadtxt(f1)\n",
    "\n",
    "trainSet_X = []\n",
    "trainSet_y = []\n",
    "# Filter out only 1s/5s\n",
    "for x in rawTrainSet:\n",
    "    if x[0]==1.0 or x[0]==5.0 :\n",
    "        trainSet_X.append(x[1:])\n",
    "        trainSet_y.append(x[0])\n",
    "#print(trainSet_X)\n",
    "\n",
    "# loead test data\n",
    "f1 = urllib.request.urlopen(\"http://www.amlbook.com/data/zip/features.test\")\n",
    "rawTestSet = np.loadtxt(f1)\n",
    "testSet_X = []\n",
    "testSet_y = []\n",
    "for x in rawTestSet:\n",
    "    if x[0]==1.0 or x[0]==5.0 :\n",
    "        testSet_X.append(x[1:])\n",
    "        testSet_y.append(x[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a)\n",
    "Using linear kernel, and all of training data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is :  97.87735849056604\n",
      "number of support vectors [Class +1, Class -1] is:  [14 14]\n"
     ]
    }
   ],
   "source": [
    "# training && accuracy reporting!\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "mySvm = svm.SVC(kernel='linear')\n",
    "mySvm.fit(trainSet_X, trainSet_y)\n",
    "\n",
    "pred = mySvm.predict(testSet_X)\n",
    "print(\"Accuracy is : \", accuracy_score(testSet_y, pred)*100)\n",
    "print(\"number of support vectors [Class +1, Class -1] is: \", mySvm.n_support_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Using first 50 data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is :  98.11320754716981 %\n",
      "number of support vectors [Class +1, Class -1] is:  [1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "mySvm = svm.SVC(kernel='linear')\n",
    "mySvm.fit(trainSet_X[:50], trainSet_y[:50])\n",
    "\n",
    "pred = mySvm.predict(testSet_X)\n",
    "print(\"Accuracy is : \", accuracy_score(testSet_y, pred)*100, \"%\")\n",
    "print(\"number of support vectors [Class +1, Class -1] is: \", mySvm.n_support_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using first 100 data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is :  98.11320754716981 %\n",
      "number of support vectors [Class +1, Class -1] is:  [2 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "mySvm = svm.SVC(kernel='linear')\n",
    "mySvm.fit(trainSet_X[:100], trainSet_y[:100])\n",
    "\n",
    "pred = mySvm.predict(testSet_X)\n",
    "print(\"Accuracy is : \", accuracy_score(testSet_y, pred)*100, \"%\")\n",
    "print(\"number of support vectors [Class +1, Class -1] is: \", mySvm.n_support_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using first 200 data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is :  98.11320754716981 %\n",
      "number of support vectors [Class +1, Class -1] is:  [4 4]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "mySvm = svm.SVC(kernel='linear')\n",
    "mySvm.fit(trainSet_X[:200], trainSet_y[:200])\n",
    "\n",
    "pred = mySvm.predict(testSet_X)\n",
    "print(\"Accuracy is : \", accuracy_score(testSet_y, pred)*100, \"%\")\n",
    "print(\"number of support vectors [Class +1, Class -1] is: \", mySvm.n_support_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using first 800 data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is :  98.11320754716981 %\n",
      "number of support vectors [Class +1, Class -1] is:  [7 7]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "mySvm = svm.SVC(kernel='linear')\n",
    "mySvm.fit(trainSet_X[:800], trainSet_y[:800])\n",
    "\n",
    "pred = mySvm.predict(testSet_X)\n",
    "print(\"Accuracy is : \", accuracy_score(testSet_y, pred)*100, \"%\")\n",
    "print(\"number of support vectors [Class +1, Class -1] is: \", mySvm.n_support_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Comparing polynomial kernels with degree = 2 and 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=  0.0001\n",
      "Training Error for degree Q = 2  is :  1.8577834721332454 %\n",
      "Training Error for degree Q = 5  is :  0.5124919923126248 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "c = 0.0001\n",
    "\n",
    "mySvm_deg2 = svm.SVC(kernel='poly', degree=2, gamma='auto', C=c, coef0=1)\n",
    "mySvm_deg2.fit(trainSet_X, trainSet_y)\n",
    "\n",
    "mySvm_deg5 = svm.SVC(kernel='poly', degree=5,gamma='auto', C=c, coef0=1)\n",
    "mySvm_deg5.fit(trainSet_X, trainSet_y)\n",
    "\n",
    "pred2 = mySvm_deg2.predict(trainSet_X)\n",
    "pred5 = mySvm_deg5.predict(trainSet_X)\n",
    "\n",
    "print(\"C= \", c)\n",
    "print(\"Training Error for degree Q = 2  is : \", (1-accuracy_score(trainSet_y, pred2))*100, \"%\")\n",
    "print(\"Training Error for degree Q = 5  is : \", (1-accuracy_score(trainSet_y, pred5))*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) **False**. Training Error is **lower** for Q=5 compared to Q=2 for C = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=  0.001\n",
      "Support Vectors for degree Q = 2  is :  142\n",
      "Support Vectors for degree Q = 5  is :  26\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "c = 0.001\n",
    "\n",
    "mySvm_deg2 = svm.SVC(kernel='poly', degree=2, gamma='auto', C=c, coef0=1)\n",
    "mySvm_deg2.fit(trainSet_X, trainSet_y)\n",
    "\n",
    "mySvm_deg5 = svm.SVC(kernel='poly', degree=5,gamma='auto', C=c, coef0=1)\n",
    "mySvm_deg5.fit(trainSet_X, trainSet_y)\n",
    "\n",
    "print(\"C= \", c)\n",
    "print(\"Support Vectors for degree Q = 2  is : \", mySvm_deg2.n_support_.sum())\n",
    "print(\"Support Vectors for degree Q = 5  is : \", mySvm_deg5.n_support_.sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) **True** Number of support vectors is lower at Q = 5 for c = 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=  0.01\n",
      "Training Error for degree Q = 2  is :  0.4484304932735439 %\n",
      "Training Error for degree Q = 5  is :  0.384368994234463 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "c = 0.01\n",
    "\n",
    "mySvm_deg2 = svm.SVC(kernel='poly', degree=2, gamma='auto', C=c, coef0=1)\n",
    "mySvm_deg2.fit(trainSet_X, trainSet_y)\n",
    "\n",
    "mySvm_deg5 = svm.SVC(kernel='poly', degree=5,gamma='auto', C=c, coef0=1)\n",
    "mySvm_deg5.fit(trainSet_X, trainSet_y)\n",
    "\n",
    "pred2 = mySvm_deg2.predict(trainSet_X)\n",
    "pred5 = mySvm_deg5.predict(trainSet_X)\n",
    "\n",
    "print(\"C= \", c)\n",
    "print(\"Training Error for degree Q = 2  is : \", (1-accuracy_score(trainSet_y, pred2))*100, \"%\")\n",
    "print(\"Training Error for degree Q = 5  is : \", (1-accuracy_score(trainSet_y, pred5))*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) ** False ** Training error is lower in case of Q=5 and Q=2 for c = 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=  1\n",
      "Test error for degree Q=2 is :  1.8867924528301883 %\n",
      "Test error for degree Q=5 is :  1.8867924528301883 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "c = 1\n",
    "\n",
    "mySvm_deg2 = svm.SVC(kernel='poly', degree=2, gamma='auto', C=c, coef0=1)\n",
    "mySvm_deg2.fit(trainSet_X, trainSet_y)\n",
    "\n",
    "mySvm_deg5 = svm.SVC(kernel='poly', degree=5,gamma='auto', C=c, coef0=1)\n",
    "mySvm_deg5.fit(trainSet_X, trainSet_y)\n",
    "\n",
    "pred2 = mySvm_deg2.predict(testSet_X)\n",
    "pred5 = mySvm_deg5.predict(testSet_X)\n",
    "\n",
    "print(\"C= \", c)\n",
    "print(\"Test error for degree Q=2 is : \",(1-accuracy_score(testSet_y, pred2))*100, \"%\")\n",
    "print(\"Test error for degree Q=5 is : \", (1-accuracy_score(testSet_y, pred5))*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4). ** False ** Test error is same @ Q=5 and Q=2 for C = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C is:  0.01\n",
      "Test Error is :  2.1226415094339646 %\n",
      "Training Error is :  0.384368994234463 %\n",
      "---------------\n",
      "C is:  1\n",
      "Test Error is :  2.1226415094339646 %\n",
      "Training Error is :  0.4484304932735439 %\n",
      "---------------\n",
      "C is:  100\n",
      "Test Error is :  1.8867924528301883 %\n",
      "Training Error is :  0.32030749519538215 %\n",
      "---------------\n",
      "C is:  10000\n",
      "Test Error is :  1.8867924528301883 %\n",
      "Training Error is :  0.2562459961563124 %\n",
      "---------------\n",
      "C is:  1000000\n",
      "Test Error is :  2.1226415094339646 %\n",
      "Training Error is :  0.12812299807815064 %\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for i in [0.01, 1, 100, 10000, 1000000]:\n",
    "    mySvm = svm.SVC(kernel='rbf', C=i, gamma='auto')\n",
    "    mySvm.fit(trainSet_X, trainSet_y)\n",
    "    print(\"C is: \", i)\n",
    "    pred = mySvm.predict(testSet_X)\n",
    "    pred_train = mySvm.predict(trainSet_X)\n",
    "    print(\"Test Error is : \", (1-accuracy_score(testSet_y, pred))*100, \"%\")\n",
    "    print(\"Training Error is : \", (1-accuracy_score(trainSet_y, pred_train))*100, \"%\")\n",
    "    print(\"---------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowest training error: ** C = 10^6 **\n",
    "\n",
    "Lowest test error: ** C = 100 or 10^4 ** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q(5)\n",
    "\n",
    "## (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n"
     ]
    }
   ],
   "source": [
    "# Loading Data!\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "\n",
    "# Open the url for reading\n",
    "f1 = open(\"gisette_train.labels\", \"r\")\n",
    "# fast reading function!\n",
    "_5_trainSet_y = np.loadtxt(f1)\n",
    "\n",
    "print(len(trainSet_y))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = open(\"gisette_valid.labels\", \"r\")\n",
    "_5_validSet_y = np.loadtxt(f1)\n",
    "\n",
    "#print(_5_validSet_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = open(\"gisette_valid.data\", \"r\")\n",
    "_5_validSet_X = np.loadtxt(f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n"
     ]
    }
   ],
   "source": [
    "f1 = open(\"gisette_train.data\", \"r\")\n",
    "_5_trainSet_X = np.loadtxt(f1)\n",
    "print(len(_5_trainSet_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "mySvm_5 = svm.SVC(kernel='linear')\n",
    "mySvm_5.fit(_5_trainSet_X, _5_trainSet_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error is :  2.400000000000002 %\n",
      "Training Error is :  0.0 %\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "pred = mySvm_5.predict(_5_validSet_X)\n",
    "pred_train = mySvm_5.predict(_5_trainSet_X)\n",
    "print(\"Test Error is : \", (1-accuracy_score(_5_validSet_y, pred))*100, \"%\")\n",
    "print(\"Training Error is : \", (1-accuracy_score(_5_trainSet_y, pred_train))*100, \"%\")\n",
    "print(\"---------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of support vectors:  1084\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of support vectors: \", mySvm_5.n_support_.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBF Kernel!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"RBF Kernel!\")\n",
    "\n",
    "mySvm_5R = svm.SVC(kernel='rbf', gamma=0.001)\n",
    "mySvm_5R.fit(_5_trainSet_X, _5_trainSet_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error is :  50.0 %\n",
      "Training Error is :  0.0 %\n",
      "Number of support vectors:  6000\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "pred = mySvm_5R.predict(_5_validSet_X)\n",
    "pred_train = mySvm_5R.predict(_5_trainSet_X)\n",
    "print(\"Test Error is : \", (1-accuracy_score(_5_validSet_y, pred))*100, \"%\")\n",
    "print(\"Training Error is : \", (1-accuracy_score(_5_trainSet_y, pred_train))*100, \"%\")\n",
    "print(\"Number of support vectors: \", mySvm_5R.n_support_.sum())\n",
    "print(\"---------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poly Kernel!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/supreet/.local/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=1,\n",
       "  decision_function_shape='ovr', degree=2, gamma='auto_deprecated',\n",
       "  kernel='poly', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Poly Kernel!\")\n",
    "\n",
    "mySvm_5P = svm.SVC(kernel='poly', degree=2, coef0=1)\n",
    "mySvm_5P.fit(_5_trainSet_X, _5_trainSet_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error is :  2.100000000000002 %\n",
      "Training Error is :  0.0 %\n",
      "Number of support vectors:  1755\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "pred = mySvm_5P.predict(_5_validSet_X)\n",
    "pred_train = mySvm_5P.predict(_5_trainSet_X)\n",
    "print(\"Test Error is : \", (1-accuracy_score(_5_validSet_y, pred))*100, \"%\")\n",
    "print(\"Training Error is : \", (1-accuracy_score(_5_trainSet_y, pred_train))*100, \"%\")\n",
    "print(\"Number of support vectors: \", mySvm_5P.n_support_.sum())\n",
    "print(\"---------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Both yield equal training error of 0%. However, on test data, rbf kernel performs very poorly ~50% accuracy, whereas polynomial kernel gives high ~98% accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a)\n",
    "\n",
    "Here, I have used the random subset of data to be chosen for training each tree to be 1/2 of total training set.\n",
    "\n",
    "* k = number of trees = 10\n",
    "\n",
    "* max_depth of any tree (to prevent infinite recursion) = 2900\n",
    "\n",
    "* Decision tree class from previous assignment has been modified slightly to suit my needs\n",
    "\n",
    "* A new class for random forests, MyRF() has been created. Its parameters are self explanatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data !\n",
    "import numpy as np\n",
    "f = open(\"spam.data\", \"r\")\n",
    "rawSet_6 = np.loadtxt(f)\n",
    "\n",
    "np.random.shuffle(rawSet_6)\n",
    "#print(rawSet_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30 % data is test data!\n",
    "test_set_6 = [x for i, x in enumerate(rawSet_6) if i % 10 == 9 or i % 10 == 6 or i%7 == 3]\n",
    "train_set_6 =  [x for i, x in enumerate(rawSet_6) if i % 10 != 9 and i % 10 != 6 and i%7 != 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not used as such, but should do it so that python list-comprehends train_set_6 and \n",
    "# does not cause recursion error\n",
    "train_set_6_X = [x[:-1] for x in (train_set_6) ]\n",
    "train_set_6_y = [x[-1] for x in (train_set_6)]\n",
    "\n",
    "test_set_6_X = [x[:-1] for x in (test_set_6) ]\n",
    "test_set_6_y = [x[-1] for x in (test_set_6)]\n",
    "#print(train_set_6_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# Total number of attribs\n",
    "M = 58\n",
    "# Number of features for split! , Last is class label.\n",
    "m2 = (math.sqrt(M-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "code_folding": [
     10,
     109,
     114
    ]
   },
   "outputs": [],
   "source": [
    "# Decision Tree code.\n",
    "#print(train_set_6)\n",
    "#print(test_set_6)\n",
    "## Code for decision Tree!\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "# Enter You Name Here\n",
    "myname = \"Supreet\" # or \"Doe-Jane-\"\n",
    "\n",
    "class Node():\n",
    "    left = None\n",
    "    right = None\n",
    "    ans = 2 # 2 means none, 0,1 -> the classification\n",
    "    attrib = 0 #index of attribute to split on\n",
    "    val = 0 #the value to split on. \n",
    "    def __init__(self, a, v, q):\n",
    "        self.attrib = a\n",
    "        self.val = v\n",
    "        self.ans = q\n",
    "        \n",
    "    def isLeaf(self): #boolean\n",
    "        if self.ans == 2:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "\n",
    "# Implement your decision tree below\n",
    "class DecisionTree():\n",
    "    tree = Node(0, 0, 2)\n",
    "    max_depth = 2900\n",
    "    depth = 0\n",
    "    \n",
    "    m_feat = -1 #Need to be set manually to >0\n",
    "    \n",
    "    def splitVal(self, data, idx): # retuurns mean\n",
    "        sum = 0.0\n",
    "        for x in data:\n",
    "            sum = sum+ float(x[idx])\n",
    "        \n",
    "        return float(float(sum)/float(len(data)))\n",
    "        \n",
    "    def entropy(self, pa, na, ntot): #returns float.\n",
    "        if pa == 1 or pa == 0:\n",
    "            return 0\n",
    "        #print(pa)\n",
    "        \n",
    "        e = float(pa) * float(math.log(pa, 2))\n",
    "        e = e + float((1-pa) * math.log(1-pa, 2))\n",
    "        \n",
    "        return float(-1)*(float(na)/float(ntot))*e\n",
    "        \n",
    "    \n",
    "    def prob1(self, data): #Probability of items to be in class 1.\n",
    "        if len(data) == 0:\n",
    "            return float(0)\n",
    "        \n",
    "        sum = 0\n",
    "        for x in data:\n",
    "            sum = sum + int(x[M-1]) # 11th index (12th elem) is the quality\n",
    "        #print(\"sum: \", sum, \"len: \", len(data))\n",
    "        g= float(float(sum)/float(len(data)))\n",
    "        #print(\"prob:\", g)\n",
    "        return g\n",
    "    \n",
    "    def learn(self, training_set): #returns a node with the split conditions.\n",
    "        # implement this function\n",
    "         \n",
    "        if len(training_set) == 0:\n",
    "            return None\n",
    "        \n",
    "        tpl = self.splitNode(training_set)\n",
    "        root = Node(tpl[1], tpl[2], 2)\n",
    "        \n",
    "        #print(\"got tuple to split: @ attrib: %d, value: %f\" % (root.attrib, root.val) )\n",
    "        \n",
    "        if self.prob1(training_set) == 1 :\n",
    "            root.ans = 1\n",
    "            return root\n",
    "        \n",
    "        elif self.prob1(training_set) == 0:\n",
    "            root.ans = 0\n",
    "            return root\n",
    "        \n",
    "        else :\n",
    "            self.depth = self.depth + 1\n",
    "            #print(self.depth)\n",
    "            l1 = [x for x in training_set if float(x[root.attrib]) <= root.val]\n",
    "            l2 = [x for x in training_set if float(x[root.attrib]) > root.val]\n",
    "            \n",
    "            #print(\"left: %d, right %d\" %(len(l1), len(l2)))\n",
    "            if np.array_equal(training_set, l1):\n",
    "                root.ans = 1 if self.prob1(l1) > 0.5 else 0\n",
    "                return root\n",
    "            elif np.array_equal(training_set, l2):\n",
    "                root.ans = 1 if self.prob1(l2) > 0.5 else 0\n",
    "                return root\n",
    "            \n",
    "            root.left = self.learn(l1)\n",
    "            root.right = self.learn(l2)\n",
    "            return root\n",
    "        '''else:\n",
    "            if self.prob1(training_set)>0.5:\n",
    "                root.ans = 1\n",
    "                return root\n",
    "            else:\n",
    "                root.ans = 0\n",
    "                return root\n",
    "        '''\n",
    "\n",
    "    # implement this function\n",
    "    def classify(self, test_instance):\n",
    "        #result = 0 # baseline: always classifies as 0\n",
    "        result = self.treeTraverse(test_instance, self.tree)\n",
    "        return result\n",
    "    \n",
    "    def treeTraverse(self, x_vector, node: Node) -> int:\n",
    "        if node is None:\n",
    "            return -1\n",
    "        elif not node.isLeaf():\n",
    "            return node.ans\n",
    "        elif float(x_vector[node.attrib]) <= node.val:\n",
    "            return self.treeTraverse(x_vector, node.left)\n",
    "        else:\n",
    "            return self.treeTraverse(x_vector, node.right)\n",
    "            \n",
    "        \n",
    "    \n",
    "    def splitNode(self, data): # returns the minimum entropy.\n",
    "        me = float(11000)\n",
    "        index = 0\n",
    "        Nm = len(data)\n",
    "        mSVal = 0.0\n",
    "        \n",
    "        # Choose a random list of features to check split on!\n",
    "        split_features = []\n",
    "        #print(self.m_feat)\n",
    "        \n",
    "        split_features = np.random.choice(np.arange(M-1), int(self.m_feat), replace=False)\n",
    "        #print(split_features)\n",
    "        for i in split_features: # 11 is the number of attributes \n",
    "            sv = self.splitVal(data, i)\n",
    "            l1 = [x for x in data if float(x[i]) <= sv]\n",
    "            l2 = [x for x in data if float(x[i]) > sv]\n",
    "            pa1 = self.prob1(l1)\n",
    "            pa2 = self.prob1(l2)\n",
    "            #print(\"## \", pa1, pa2)\n",
    "            eThis = self.entropy(pa1, len(l1), Nm) + self.entropy(pa2, len(l2), Nm)\n",
    "            #print(\"Entropy got: %f, me was: \" % eThis, me)\n",
    "            #print(eThis < me)\n",
    "                \n",
    "            if eThis < me:\n",
    "                me = eThis\n",
    "                index = i\n",
    "                mSVal = sv\n",
    "            '''\n",
    "            me = eThis if eThis < me else me\n",
    "            index = i if eThis < me else index\n",
    "            print(\"inedx now is: %d\" % index)\n",
    "            mSVal = sv if eThis < me else mSVal\n",
    "            '''\n",
    "            #min = eThis < min ? eThis : min\n",
    "        return (me, index, mSVal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class MyRF():\n",
    "    #k = 10 # Number of trees.\n",
    "    #subSize = 20 # Size of subset of training points!\n",
    "    #trees = []\n",
    "    #m \n",
    "    def __init__(self, k, subSize, features_to_split):\n",
    "        self.k = k\n",
    "        self.subSize = int(subSize)\n",
    "        self.m = int(features_to_split)\n",
    "        self.trees=[]\n",
    "        self.oobError = 0.0\n",
    "    \n",
    "    def learnForest(self, data):\n",
    "        for i in range(self.k):\n",
    "            tree = DecisionTree()\n",
    "            tree.m_feat = int(self.m)\n",
    "            #tree.tree = tree.learn(random.choices(data, k=self.subSize))\n",
    "            np.random.shuffle(data)\n",
    "            tree.tree = tree.learn(data[:self.subSize])\n",
    "            \n",
    "            results = []\n",
    "            for instance in data[self.subSize:]:\n",
    "                result = tree.classify( instance[:-1] )\n",
    "                results.append( (result) == int(instance[-1]))\n",
    "            self.oobError += float(results.count(False)/float(len(results)))\n",
    "            self.trees.append(tree)\n",
    "    \n",
    "    def classify(self, dataPoint): # data point is without the class label!\n",
    "        n1 = 0\n",
    "        \n",
    "        for i in range(self.k):\n",
    "            result = self.trees[i].classify(dataPoint)\n",
    "            n1 = n1 + result\n",
    "        \n",
    "        if n1 >= int(self.k/2): # Majority Voting!!\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def getOOBError(self):\n",
    "        return float(self.oobError/self.k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = MyRF(10, subSize=len(train_set_6)/1.25, features_to_split=int(m2))\n",
    "rf.learnForest(train_set_6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9544\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for instance in test_set_6:\n",
    "    result = rf.classify( instance[:-1] )\n",
    "    results.append( (result) == int(instance[-1]))\n",
    "    \n",
    "accuracy = float(results.count(True))/float(len(results))\n",
    "print (\"accuracy: %.4f\" % accuracy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing with sklearn's random forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.9426399447131997\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "clf.fit(train_set_6_X, train_set_6_y)\n",
    "\n",
    "pred = clf.predict(test_set_6_X)\n",
    "\n",
    "print(\"accuracy: \", accuracy_score(test_set_6_y, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My implementation gave **higher** accuracy compared to sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)\n",
    "\n",
    "Sensitivity to 'm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For m =  1.8874586088176875 -> Accuracy = 0.4734\n",
      "-------------------------\n",
      "For m =  2.516611478423583 -> Accuracy = 0.6980\n",
      "-------------------------\n",
      "For m =  3.774917217635375 -> Accuracy = 0.8334\n",
      "-------------------------\n",
      "For m =  7.54983443527075 -> Accuracy = 0.9274\n",
      "-------------------------\n",
      "For m =  15.0996688705415 -> Accuracy = 0.9502\n",
      "-------------------------\n",
      "For m =  22.64950330581225 -> Accuracy = 0.9523\n",
      "-------------------------\n",
      "For m =  30.199337741083 -> Accuracy = 0.9509\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "m_vals = [m2/4,m2/3, m2/2, m2, 2*m2, 3*m2, 4*m2]\n",
    "\n",
    "for md in m_vals:\n",
    "    rf =  MyRF(10, subSize=len(train_set_6)/2, features_to_split=int(md))\n",
    "    rf.learnForest(train_set_6)\n",
    "    results2 = []\n",
    "    acc = []\n",
    "    i=0\n",
    "    for instance2 in test_set_6:\n",
    "        result = rf.classify( instance2[:-1] )\n",
    "        #print(\"ithpoint: \", i, \"as: \", result)\n",
    "        #i+=1\n",
    "        results2.append( (result) == int(instance2[-1]))\n",
    "    #print((results.count(True)))\n",
    "    \n",
    "    accuracy2 = float(results2.count(True))/float(len(results2))\n",
    "    print (\"For m = \", md,\"-> Accuracy = %.4f\" % accuracy2)\n",
    "    print(\"-------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
