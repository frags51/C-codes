{
 "cells": [
  {
   "attachments": {
    "aml1.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AML Assignment 1\n",
    "# Supreet Singh - CS16BTECH11038\n",
    "## (Please run using python3).\n",
    "\n",
    "## Q1.\n",
    "\n",
    "### (a) \n",
    "Training error decreases as K is varied from N to 1. At K=1, training error = 0, because the nearest distance point is that point itself.\n",
    "\n",
    "### (b)\n",
    "Generalisation error first decreases, then increases as K is varied from N to 1. At K=N, it would take close to the global maximum vote for classification, and at K=1, it would fit noisy data also. \n",
    "\n",
    "![aml1.png](attachment:aml1.png)\n",
    "\n",
    "### (c)\n",
    "1. At higher dimensions, euclidean distance is almost meaningless. It is highly likely that the distance to the closest point and the farthest point (from a given point) are approximately equal in magnitude, thus making the concept of nearest neighbor meaningless. \n",
    "2. In higher dimensions, some dimensions may not be relevant to classification at all, but still their effect would appear in the model and may lead to wrong classifications.\n",
    "\n",
    "### (d)\n",
    "It is not possible to build a decision tree that classifies the same as 1-NN, because 1-NN makes decision boundaries that are  Voronoi Diagrams (curved boundaries), whereas decision trees make boundaries which are lines parallel to the axes. Since their decision boundaries are not same, the cannot classifiy exactly the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\n",
    "Naive Baye's assumption: Each attribute occurs independently, and each class is equally likely,\n",
    "\n",
    "and $$ \\begin{equation*} P(politics | X)  = \\frac{P(X|politics).P(politics)}{P(X)} \\end{equation*} $$\n",
    "where P(politics) = 1/2, \n",
    "\n",
    "probability that the document is from politics (after simplification) is: $ \\frac{5^{4}.4}{3^{8}} $ = 0.381"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3.\n",
    "At each step, we need to maximize the information gain, or minimize the combined entropy of children nodes. \n",
    "\n",
    "### (a) Note: Written in Python3.\n",
    "(The first cell below this contains the code common to each run with different test sets for cross validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CS6510 HW 1 Code Skeleton\n",
    "# Please use this outline to implement your decision tree. You can add any code around this.\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "import math\n",
    "\n",
    "# Enter You Name Here\n",
    "myname = \"Supreet\" # or \"Doe-Jane-\"\n",
    "\n",
    "class Node():\n",
    "    left = None\n",
    "    right = None\n",
    "    ans = 2 # 2 means none, 0,1 -> the classification\n",
    "    attrib = 0 #index of attribute to split on\n",
    "    val = 0 #the value to split on. \n",
    "    def __init__(self, a, v, q):\n",
    "        self.attrib = a\n",
    "        self.val = v\n",
    "        self.ans = q\n",
    "        \n",
    "    def isLeaf(self): #boolean\n",
    "        if self.ans == 2:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "\n",
    "# Implement your decision tree below\n",
    "class DecisionTree():\n",
    "    tree = Node(0, 0, 2)\n",
    "    \n",
    "    def splitVal(self, data, idx): # retuurns mean\n",
    "        sum = 0.0\n",
    "        for x in data:\n",
    "            sum = sum+ float(x[idx])\n",
    "        \n",
    "        return float(float(sum)/float(len(data)))\n",
    "        \n",
    "    def entropy(self, pa, na, ntot): #returns float.\n",
    "        if pa == 1 or pa == 0:\n",
    "            return 0\n",
    "        \n",
    "        \n",
    "        e = float(pa) * float(math.log(pa, 2))\n",
    "        e = e + float((1-pa) * math.log(1-pa, 2))\n",
    "        \n",
    "        return float(-1)*(float(na)/float(ntot))*e\n",
    "        \n",
    "    \n",
    "    def prob1(self, data): #Probability of items to be in class 1.\n",
    "        if len(data) == 0:\n",
    "            return float(0)\n",
    "        \n",
    "        sum = 0\n",
    "        for x in data:\n",
    "            sum = sum + int(x[11]) # 11th index (12th elem) is the quality\n",
    "        \n",
    "        g= float(float(sum)/float(len(data)))\n",
    "        return g\n",
    "    \n",
    "    def learn(self, training_set): #returns a node with the split conditions.\n",
    "        # implement this function\n",
    "         \n",
    "        if len(training_set) == 0:\n",
    "            return None\n",
    "        \n",
    "        tpl = self.splitNode(training_set)\n",
    "        root = Node(tpl[1], tpl[2], 2)\n",
    "        \n",
    "        #print(\"got tuple to split: @ attrib: %d, value: %f\" % (root.attrib, root.val) )\n",
    "        \n",
    "        if self.prob1(training_set) == 1 :\n",
    "            root.ans = 1\n",
    "            return root\n",
    "        \n",
    "        elif self.prob1(training_set) == 0:\n",
    "            root.ans = 0\n",
    "            return root\n",
    "        \n",
    "        else:\n",
    "            l1 = [x for x in training_set if float(x[root.attrib]) <= root.val]\n",
    "            l2 = [x for x in training_set if float(x[root.attrib]) > root.val]\n",
    "            #print(\"left: %d, right %d\" %(len(l1), len(l2)))\n",
    "            root.left = self.learn(l1)\n",
    "            root.right = self.learn(l2)\n",
    "            return root\n",
    "\n",
    "    # implement this function\n",
    "    def classify(self, test_instance):\n",
    "        #result = 0 # baseline: always classifies as 0\n",
    "        result = self.treeTraverse(test_instance, self.tree)\n",
    "        return result\n",
    "    \n",
    "    def treeTraverse(self, x_vector, node: Node) -> int:\n",
    "        if node is None:\n",
    "            return -1\n",
    "        elif not node.isLeaf():\n",
    "            return node.ans\n",
    "        elif float(x_vector[node.attrib]) <= node.val:\n",
    "            return self.treeTraverse(x_vector, node.left)\n",
    "        else:\n",
    "            return self.treeTraverse(x_vector, node.right)\n",
    "            \n",
    "        \n",
    "    \n",
    "    def splitNode(self, data): # returns the minimum entropy.\n",
    "        me = float(11000)\n",
    "        index = 0\n",
    "        Nm = len(data)\n",
    "        mSVal = 0.0\n",
    "        \n",
    "        for i in range(11): # 11 is the number of attributes \n",
    "            sv = self.splitVal(data, i)\n",
    "            l1 = [x for x in data if float(x[i]) <= sv]\n",
    "            l2 = [x for x in data if float(x[i]) > sv]\n",
    "            pa1 = self.prob1(l1)\n",
    "            pa2 = self.prob1(l2)\n",
    "            \n",
    "            eThis = self.entropy(pa1, len(l1), Nm) + self.entropy(pa2, len(l2), Nm)\n",
    "            #print(\"Entropy got: %f, me was: \" % eThis, me)\n",
    "            #print(eThis < me)\n",
    "                \n",
    "            if eThis < me:\n",
    "                me = eThis\n",
    "                index = i\n",
    "                mSVal = sv\n",
    "            '''\n",
    "            me = eThis if eThis < me else me\n",
    "            index = i if eThis < me else index\n",
    "            print(\"inedx now is: %d\" % index)\n",
    "            mSVal = sv if eThis < me else mSVal\n",
    "            '''\n",
    "            \n",
    "            #min = eThis < min ? eThis : min\n",
    "        return (me, index, mSVal)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 4898\n",
      "accuracy: 0.8323\n"
     ]
    }
   ],
   "source": [
    "def run_decision_tree():\n",
    "\n",
    "    # Load data set\n",
    "    with open(\"wine-dataset.csv\") as f:\n",
    "        next(f, None)\n",
    "        data = [tuple(line) for line in csv.reader(f, delimiter=\",\")]\n",
    "    print (\"Number of records: %d\" % (len(data)))\n",
    "\n",
    "    # Split training/test sets\n",
    "    # You need to modify the following code for cross validation.\n",
    "    K = 10\n",
    "    training_set = [x for i, x in enumerate(data) if i % K != 9]\n",
    "    test_set = [x for i, x in enumerate(data) if i % K == 9]\n",
    "    \n",
    "    tree = DecisionTree()\n",
    "    # Construct a tree using training set\n",
    "    tree.tree = tree.learn( training_set )\n",
    "\n",
    "    # Classify the test set using the tree we just constructed\n",
    "    results = []\n",
    "    for instance in test_set:\n",
    "        result = tree.classify( instance[:-1] )\n",
    "        results.append( (result) == int(instance[-1]))\n",
    "        #print (\"Actual: %d, classified as: %d\" %(int(instance[-1]), int(result)))\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = float(results.count(True))/float(len(results))\n",
    "    print (\"accuracy: %.4f\" % accuracy)       \n",
    "    \n",
    "\n",
    "    # Writing results to a file (DO NOT CHANGE)\n",
    "    f = open(myname+\"result.txt\", \"w\")\n",
    "    f.write(\"accuracy: %.4f\" % accuracy)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_decision_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of records: 4898\n",
    "\n",
    "### accuracy: 0.8323"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Cross Validation.\n",
    "Part (a) has been run on 1 set of data. To run these, you need to run the first cell in part (a). \n",
    "\n",
    "Below is the code for the other the remaining 9 parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 4898\n",
      "accuracy: 0.8323\n"
     ]
    }
   ],
   "source": [
    "def run_decision_tree():\n",
    "\n",
    "    # Load data set\n",
    "    with open(\"wine-dataset.csv\") as f:\n",
    "        next(f, None)\n",
    "        data = [tuple(line) for line in csv.reader(f, delimiter=\",\")]\n",
    "    print (\"Number of records: %d\" % (len(data)))\n",
    "\n",
    "    # Split training/test sets\n",
    "    # You need to modify the following code for cross validation.\n",
    "    K = 10\n",
    "    training_set = [x for i, x in enumerate(data) if i % K != 8]\n",
    "    test_set = [x for i, x in enumerate(data) if i % K == 8]\n",
    "    \n",
    "    tree = DecisionTree()\n",
    "    # Construct a tree using training set\n",
    "    tree.tree = tree.learn( training_set )\n",
    "\n",
    "    # Classify the test set using the tree we just constructed\n",
    "    results = []\n",
    "    for instance in test_set:\n",
    "        result = tree.classify( instance[:-1] )\n",
    "        results.append( (result) == int(instance[-1]))\n",
    "        #print (\"Actual: %d, classified as: %d\" %(int(instance[-1]), int(result)))\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = float(results.count(True))/float(len(results))\n",
    "    print (\"accuracy: %.4f\" % accuracy)       \n",
    "    \n",
    "\n",
    "    # Writing results to a file (DO NOT CHANGE)\n",
    "    f = open(myname+\"result.txt\", \"w\")\n",
    "    f.write(\"accuracy: %.4f\" % accuracy)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_decision_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 4898\n",
      "accuracy: 0.8306\n"
     ]
    }
   ],
   "source": [
    "def run_decision_tree():\n",
    "\n",
    "    # Load data set\n",
    "    with open(\"wine-dataset.csv\") as f:\n",
    "        next(f, None)\n",
    "        data = [tuple(line) for line in csv.reader(f, delimiter=\",\")]\n",
    "    print (\"Number of records: %d\" % (len(data)))\n",
    "\n",
    "    # Split training/test sets\n",
    "    # You need to modify the following code for cross validation.\n",
    "    K = 10\n",
    "    training_set = [x for i, x in enumerate(data) if i % K != 7]\n",
    "    test_set = [x for i, x in enumerate(data) if i % K == 7]\n",
    "    \n",
    "    tree = DecisionTree()\n",
    "    # Construct a tree using training set\n",
    "    tree.tree = tree.learn( training_set )\n",
    "\n",
    "    # Classify the test set using the tree we just constructed\n",
    "    results = []\n",
    "    for instance in test_set:\n",
    "        result = tree.classify( instance[:-1] )\n",
    "        results.append( (result) == int(instance[-1]))\n",
    "        #print (\"Actual: %d, classified as: %d\" %(int(instance[-1]), int(result)))\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = float(results.count(True))/float(len(results))\n",
    "    print (\"accuracy: %.4f\" % accuracy)       \n",
    "    \n",
    "\n",
    "    # Writing results to a file (DO NOT CHANGE)\n",
    "    f = open(myname+\"result.txt\", \"w\")\n",
    "    f.write(\"accuracy: %.4f\" % accuracy)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_decision_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 4898\n",
      "accuracy: 0.8388\n"
     ]
    }
   ],
   "source": [
    "def run_decision_tree():\n",
    "\n",
    "    # Load data set\n",
    "    with open(\"wine-dataset.csv\") as f:\n",
    "        next(f, None)\n",
    "        data = [tuple(line) for line in csv.reader(f, delimiter=\",\")]\n",
    "    print (\"Number of records: %d\" % (len(data)))\n",
    "\n",
    "    # Split training/test sets\n",
    "    # You need to modify the following code for cross validation.\n",
    "    K = 10\n",
    "    training_set = [x for i, x in enumerate(data) if i % K != 6]\n",
    "    test_set = [x for i, x in enumerate(data) if i % K == 6]\n",
    "    \n",
    "    tree = DecisionTree()\n",
    "    # Construct a tree using training set\n",
    "    tree.tree = tree.learn( training_set )\n",
    "\n",
    "    # Classify the test set using the tree we just constructed\n",
    "    results = []\n",
    "    for instance in test_set:\n",
    "        result = tree.classify( instance[:-1] )\n",
    "        results.append( (result) == int(instance[-1]))\n",
    "        #print (\"Actual: %d, classified as: %d\" %(int(instance[-1]), int(result)))\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = float(results.count(True))/float(len(results))\n",
    "    print (\"accuracy: %.4f\" % accuracy)       \n",
    "    \n",
    "\n",
    "    # Writing results to a file (DO NOT CHANGE)\n",
    "    f = open(myname+\"result.txt\", \"w\")\n",
    "    f.write(\"accuracy: %.4f\" % accuracy)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_decision_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 4898\n",
      "accuracy: 0.8245\n"
     ]
    }
   ],
   "source": [
    "def run_decision_tree():\n",
    "\n",
    "    # Load data set\n",
    "    with open(\"wine-dataset.csv\") as f:\n",
    "        next(f, None)\n",
    "        data = [tuple(line) for line in csv.reader(f, delimiter=\",\")]\n",
    "    print (\"Number of records: %d\" % (len(data)))\n",
    "\n",
    "    # Split training/test sets\n",
    "    # You need to modify the following code for cross validation.\n",
    "    K = 10\n",
    "    training_set = [x for i, x in enumerate(data) if i % K != 5]\n",
    "    test_set = [x for i, x in enumerate(data) if i % K == 5]\n",
    "    \n",
    "    tree = DecisionTree()\n",
    "    # Construct a tree using training set\n",
    "    tree.tree = tree.learn( training_set )\n",
    "\n",
    "    # Classify the test set using the tree we just constructed\n",
    "    results = []\n",
    "    for instance in test_set:\n",
    "        result = tree.classify( instance[:-1] )\n",
    "        results.append( (result) == int(instance[-1]))\n",
    "        #print (\"Actual: %d, classified as: %d\" %(int(instance[-1]), int(result)))\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = float(results.count(True))/float(len(results))\n",
    "    print (\"accuracy: %.4f\" % accuracy)       \n",
    "    \n",
    "\n",
    "    # Writing results to a file (DO NOT CHANGE)\n",
    "    f = open(myname+\"result.txt\", \"w\")\n",
    "    f.write(\"accuracy: %.4f\" % accuracy)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_decision_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 4898\n",
      "accuracy: 0.8531\n"
     ]
    }
   ],
   "source": [
    "def run_decision_tree():\n",
    "\n",
    "    # Load data set\n",
    "    with open(\"wine-dataset.csv\") as f:\n",
    "        next(f, None)\n",
    "        data = [tuple(line) for line in csv.reader(f, delimiter=\",\")]\n",
    "    print (\"Number of records: %d\" % (len(data)))\n",
    "\n",
    "    # Split training/test sets\n",
    "    # You need to modify the following code for cross validation.\n",
    "    K = 10\n",
    "    training_set = [x for i, x in enumerate(data) if i % K != 4]\n",
    "    test_set = [x for i, x in enumerate(data) if i % K == 4]\n",
    "    \n",
    "    tree = DecisionTree()\n",
    "    # Construct a tree using training set\n",
    "    tree.tree = tree.learn( training_set )\n",
    "\n",
    "    # Classify the test set using the tree we just constructed\n",
    "    results = []\n",
    "    for instance in test_set:\n",
    "        result = tree.classify( instance[:-1] )\n",
    "        results.append( (result) == int(instance[-1]))\n",
    "        #print (\"Actual: %d, classified as: %d\" %(int(instance[-1]), int(result)))\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = float(results.count(True))/float(len(results))\n",
    "    print (\"accuracy: %.4f\" % accuracy)       \n",
    "    \n",
    "\n",
    "    # Writing results to a file (DO NOT CHANGE)\n",
    "    f = open(myname+\"result.txt\", \"w\")\n",
    "    f.write(\"accuracy: %.4f\" % accuracy)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_decision_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 4898\n",
      "accuracy: 0.8327\n"
     ]
    }
   ],
   "source": [
    "def run_decision_tree():\n",
    "\n",
    "    # Load data set\n",
    "    with open(\"wine-dataset.csv\") as f:\n",
    "        next(f, None)\n",
    "        data = [tuple(line) for line in csv.reader(f, delimiter=\",\")]\n",
    "    print (\"Number of records: %d\" % (len(data)))\n",
    "\n",
    "    # Split training/test sets\n",
    "    # You need to modify the following code for cross validation.\n",
    "    K = 10\n",
    "    training_set = [x for i, x in enumerate(data) if i % K != 3]\n",
    "    test_set = [x for i, x in enumerate(data) if i % K == 3]\n",
    "    \n",
    "    tree = DecisionTree()\n",
    "    # Construct a tree using training set\n",
    "    tree.tree = tree.learn( training_set )\n",
    "\n",
    "    # Classify the test set using the tree we just constructed\n",
    "    results = []\n",
    "    for instance in test_set:\n",
    "        result = tree.classify( instance[:-1] )\n",
    "        results.append( (result) == int(instance[-1]))\n",
    "        #print (\"Actual: %d, classified as: %d\" %(int(instance[-1]), int(result)))\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = float(results.count(True))/float(len(results))\n",
    "    print (\"accuracy: %.4f\" % accuracy)       \n",
    "    \n",
    "\n",
    "    # Writing results to a file (DO NOT CHANGE)\n",
    "    f = open(myname+\"result.txt\", \"w\")\n",
    "    f.write(\"accuracy: %.4f\" % accuracy)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_decision_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 4898\n",
      "accuracy: 0.8286\n"
     ]
    }
   ],
   "source": [
    "def run_decision_tree():\n",
    "\n",
    "    # Load data set\n",
    "    with open(\"wine-dataset.csv\") as f:\n",
    "        next(f, None)\n",
    "        data = [tuple(line) for line in csv.reader(f, delimiter=\",\")]\n",
    "    print (\"Number of records: %d\" % (len(data)))\n",
    "\n",
    "    # Split training/test sets\n",
    "    # You need to modify the following code for cross validation.\n",
    "    K = 10\n",
    "    training_set = [x for i, x in enumerate(data) if i % K != 2]\n",
    "    test_set = [x for i, x in enumerate(data) if i % K == 2]\n",
    "    \n",
    "    tree = DecisionTree()\n",
    "    # Construct a tree using training set\n",
    "    tree.tree = tree.learn( training_set )\n",
    "\n",
    "    # Classify the test set using the tree we just constructed\n",
    "    results = []\n",
    "    for instance in test_set:\n",
    "        result = tree.classify( instance[:-1] )\n",
    "        results.append( (result) == int(instance[-1]))\n",
    "        #print (\"Actual: %d, classified as: %d\" %(int(instance[-1]), int(result)))\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = float(results.count(True))/float(len(results))\n",
    "    print (\"accuracy: %.4f\" % accuracy)       \n",
    "    \n",
    "\n",
    "    # Writing results to a file (DO NOT CHANGE)\n",
    "    f = open(myname+\"result.txt\", \"w\")\n",
    "    f.write(\"accuracy: %.4f\" % accuracy)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_decision_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 4898\n",
      "accuracy: 0.8469\n"
     ]
    }
   ],
   "source": [
    "def run_decision_tree():\n",
    "\n",
    "    # Load data set\n",
    "    with open(\"wine-dataset.csv\") as f:\n",
    "        next(f, None)\n",
    "        data = [tuple(line) for line in csv.reader(f, delimiter=\",\")]\n",
    "    print (\"Number of records: %d\" % (len(data)))\n",
    "\n",
    "    # Split training/test sets\n",
    "    # You need to modify the following code for cross validation.\n",
    "    K = 10\n",
    "    training_set = [x for i, x in enumerate(data) if i % K != 1]\n",
    "    test_set = [x for i, x in enumerate(data) if i % K == 1]\n",
    "    \n",
    "    tree = DecisionTree()\n",
    "    # Construct a tree using training set\n",
    "    tree.tree = tree.learn( training_set )\n",
    "\n",
    "    # Classify the test set using the tree we just constructed\n",
    "    results = []\n",
    "    for instance in test_set:\n",
    "        result = tree.classify( instance[:-1] )\n",
    "        results.append( (result) == int(instance[-1]))\n",
    "        #print (\"Actual: %d, classified as: %d\" %(int(instance[-1]), int(result)))\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = float(results.count(True))/float(len(results))\n",
    "    print (\"accuracy: %.4f\" % accuracy)       \n",
    "    \n",
    "\n",
    "    # Writing results to a file (DO NOT CHANGE)\n",
    "    f = open(myname+\"result.txt\", \"w\")\n",
    "    f.write(\"accuracy: %.4f\" % accuracy)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_decision_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 4898\n",
      "accuracy: 0.8408\n"
     ]
    }
   ],
   "source": [
    "def run_decision_tree():\n",
    "\n",
    "    # Load data set\n",
    "    with open(\"wine-dataset.csv\") as f:\n",
    "        next(f, None)\n",
    "        data = [tuple(line) for line in csv.reader(f, delimiter=\",\")]\n",
    "    print (\"Number of records: %d\" % (len(data)))\n",
    "\n",
    "    # Split training/test sets\n",
    "    # You need to modify the following code for cross validation.\n",
    "    K = 10\n",
    "    training_set = [x for i, x in enumerate(data) if i % K != 0]\n",
    "    test_set = [x for i, x in enumerate(data) if i % K == 0]\n",
    "    \n",
    "    tree = DecisionTree()\n",
    "    # Construct a tree using training set\n",
    "    tree.tree = tree.learn( training_set )\n",
    "\n",
    "    # Classify the test set using the tree we just constructed\n",
    "    results = []\n",
    "    for instance in test_set:\n",
    "        result = tree.classify( instance[:-1] )\n",
    "        results.append( (result) == int(instance[-1]))\n",
    "        #print (\"Actual: %d, classified as: %d\" %(int(instance[-1]), int(result)))\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = float(results.count(True))/float(len(results))\n",
    "    print (\"accuracy: %.4f\" % accuracy)       \n",
    "    \n",
    "\n",
    "    # Writing results to a file (DO NOT CHANGE)\n",
    "    f = open(myname+\"result.txt\", \"w\")\n",
    "    f.write(\"accuracy: %.4f\" % accuracy)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_decision_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average of these 10 fold errors:\n",
    "= 0.83647"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\n",
    "\n",
    "#### Using gini index: (This cell is standalone)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 4898\n",
      "accuracy: 0.8531\n"
     ]
    }
   ],
   "source": [
    "# CS6510 HW 1 Code Skeleton\n",
    "# Please use this outline to implement your decision tree. You can add any code around this.\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "import math\n",
    "\n",
    "# Enter You Name Here\n",
    "myname = \"Supreet\" # or \"Doe-Jane-\"\n",
    "\n",
    "class Node():\n",
    "    left = None\n",
    "    right = None\n",
    "    ans = 2 # 2 means none, 0,1 -> the classification\n",
    "    attrib = 0 #index of attribute to split on\n",
    "    val = 0 #the value to split on. \n",
    "    def __init__(self, a, v, q):\n",
    "        self.attrib = a\n",
    "        self.val = v\n",
    "        self.ans = q\n",
    "        \n",
    "    def isLeaf(self): #boolean\n",
    "        if self.ans == 2:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "\n",
    "# Implement your decision tree below\n",
    "class DecisionTree():\n",
    "    tree = Node(0, 0, 2)\n",
    "    \n",
    "    def splitVal(self, data, idx): # retuurns mean\n",
    "        sum = 0.0\n",
    "        for x in data:\n",
    "            sum = sum+ float(x[idx])\n",
    "        \n",
    "        return float(float(sum)/float(len(data)))\n",
    "        \n",
    "    def entropy(self, pa, na, ntot): #Actully Gini index\n",
    "        if pa == 1 or pa == 0:\n",
    "            return 0\n",
    "        \n",
    "        e = float(2)*pa*(1-pa)\n",
    "       \n",
    "        \n",
    "        return float(1)*(float(na)/float(ntot))*e\n",
    "        \n",
    "    \n",
    "    def prob1(self, data): #Probability of items to be in class 1.\n",
    "        if len(data) == 0:\n",
    "            return float(0)\n",
    "        \n",
    "        sum = 0\n",
    "        for x in data:\n",
    "            sum = sum + int(x[11]) # 11th index (12th elem) is the quality\n",
    "        \n",
    "        g= float(float(sum)/float(len(data)))\n",
    "        return g\n",
    "    \n",
    "    def learn(self, training_set): #returns a node with the split conditions.\n",
    "        # implement this function\n",
    "         \n",
    "        if len(training_set) == 0:\n",
    "            return None\n",
    "        \n",
    "        tpl = self.splitNode(training_set)\n",
    "        root = Node(tpl[1], tpl[2], 2)\n",
    "        \n",
    "        #print(\"got tuple to split: @ attrib: %d, value: %f\" % (root.attrib, root.val) )\n",
    "        \n",
    "        if self.prob1(training_set) == 1 :\n",
    "            root.ans = 1\n",
    "            return root\n",
    "        \n",
    "        elif self.prob1(training_set) == 0:\n",
    "            root.ans = 0\n",
    "            return root\n",
    "        \n",
    "        else:\n",
    "            l1 = [x for x in training_set if float(x[root.attrib]) <= root.val]\n",
    "            l2 = [x for x in training_set if float(x[root.attrib]) > root.val]\n",
    "            #print(\"left: %d, right %d\" %(len(l1), len(l2)))\n",
    "            root.left = self.learn(l1)\n",
    "            root.right = self.learn(l2)\n",
    "            return root\n",
    "\n",
    "    # implement this function\n",
    "    def classify(self, test_instance):\n",
    "        #result = 0 # baseline: always classifies as 0\n",
    "        result = self.treeTraverse(test_instance, self.tree)\n",
    "        return result\n",
    "    \n",
    "    def treeTraverse(self, x_vector, node: Node) -> int:\n",
    "        if node is None:\n",
    "            return -1\n",
    "        elif not node.isLeaf():\n",
    "            return node.ans\n",
    "        elif float(x_vector[node.attrib]) <= node.val:\n",
    "            return self.treeTraverse(x_vector, node.left)\n",
    "        else:\n",
    "            return self.treeTraverse(x_vector, node.right)\n",
    "            \n",
    "        \n",
    "    \n",
    "    def splitNode(self, data): # returns the minimum entropy.\n",
    "        me = float(11000)\n",
    "        index = 0\n",
    "        Nm = len(data)\n",
    "        mSVal = 0.0\n",
    "        \n",
    "        for i in range(11): # 11 is the number of attributes \n",
    "            sv = self.splitVal(data, i)\n",
    "            l1 = [x for x in data if float(x[i]) <= sv]\n",
    "            l2 = [x for x in data if float(x[i]) > sv]\n",
    "            pa1 = self.prob1(l1)\n",
    "            pa2 = self.prob1(l2)\n",
    "            \n",
    "            eThis = self.entropy(pa1, len(l1), Nm) + self.entropy(pa2, len(l2), Nm)\n",
    "            #print(\"Entropy got: %f, me was: \" % eThis, me)\n",
    "            #print(eThis < me)\n",
    "                \n",
    "            if eThis < me:\n",
    "                me = eThis\n",
    "                index = i\n",
    "                mSVal = sv\n",
    "            '''\n",
    "            me = eThis if eThis < me else me\n",
    "            index = i if eThis < me else index\n",
    "            print(\"inedx now is: %d\" % index)\n",
    "            mSVal = sv if eThis < me else mSVal\n",
    "            '''\n",
    "            \n",
    "            #min = eThis < min ? eThis : min\n",
    "        return (me, index, mSVal)\n",
    "    \n",
    "def run_decision_tree():\n",
    "\n",
    "    # Load data set\n",
    "    with open(\"wine-dataset.csv\") as f:\n",
    "        next(f, None)\n",
    "        data = [tuple(line) for line in csv.reader(f, delimiter=\",\")]\n",
    "    print (\"Number of records: %d\" % (len(data)))\n",
    "\n",
    "    # Split training/test sets\n",
    "    # You need to modify the following code for cross validation.\n",
    "    K = 10\n",
    "    training_set = [x for i, x in enumerate(data) if i % K != 1]\n",
    "    test_set = [x for i, x in enumerate(data) if i % K == 1]\n",
    "    \n",
    "    tree = DecisionTree()\n",
    "    # Construct a tree using training set\n",
    "    tree.tree = tree.learn( training_set )\n",
    "\n",
    "    # Classify the test set using the tree we just constructed\n",
    "    results = []\n",
    "    for instance in test_set:\n",
    "        result = tree.classify( instance[:-1] )\n",
    "        results.append( (result) == int(instance[-1]))\n",
    "        #print (\"Actual: %d, classified as: %d\" %(int(instance[-1]), int(result)))\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = float(results.count(True))/float(len(results))\n",
    "    print (\"accuracy: %.4f\" % accuracy)       \n",
    "    \n",
    "\n",
    "    # Writing results to a file (DO NOT CHANGE)\n",
    "    f = open(myname+\"result.txt\", \"w\")\n",
    "    f.write(\"accuracy: %.4f\" % accuracy)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_decision_tree()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compared to entropy, using gini index resulted in very slightly better accuracy (0.8531 - gini vs 0.84 - entropy).\n",
    "\n",
    "However, computation time taken using gini index is lower, because logarithms take more time to compute. Also, in this case (2 classes), a closed form solution : $ gini = 2.P(a).(1-P(a)) $ exists. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre - Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 4898\n",
      "accuracy: 0.8245\n"
     ]
    }
   ],
   "source": [
    "# CS6510 HW 1 Code Skeleton\n",
    "# Please use this outline to implement your decision tree. You can add any code around this.\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "import math\n",
    "\n",
    "# Enter You Name Here\n",
    "myname = \"Supreet\" # or \"Doe-Jane-\"\n",
    "\n",
    "class Node():\n",
    "    left = None\n",
    "    right = None\n",
    "    ans = 2 # 2 means none, 0,1 -> the classification\n",
    "    attrib = 0 #index of attribute to split on\n",
    "    val = 0 #the value to split on. \n",
    "    \n",
    "    class1 = 0\n",
    "    class0 = 0\n",
    "    \n",
    "    def __init__(self, a, v, q):\n",
    "        self.attrib = a\n",
    "        self.val = v\n",
    "        self.ans = q\n",
    "        \n",
    "    def isLeaf(self): #boolean\n",
    "        if self.ans == 2:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "\n",
    "# Implement your decision tree below\n",
    "class DecisionTree():\n",
    "    tree = Node(0, 0, 2)\n",
    "    \n",
    "    def splitVal(self, data, idx): # retuurns mean\n",
    "        sum = 0.0\n",
    "        for x in data:\n",
    "            sum = sum+ float(x[idx])\n",
    "        \n",
    "        return float(float(sum)/float(len(data)))\n",
    "        \n",
    "    def entropy(self, pa, na, ntot): #Actully Gini index\n",
    "        if pa == 1 or pa == 0:\n",
    "            return 0\n",
    "        \n",
    "        e = float(2)*pa*(1-pa)\n",
    "       \n",
    "        \n",
    "        return float(1)*(float(na)/float(ntot))*e\n",
    "        \n",
    "    \n",
    "    def prob1(self, data): #Probability of items to be in class 1.\n",
    "        if len(data) == 0:\n",
    "            return float(0)\n",
    "        \n",
    "        sum = 0\n",
    "        for x in data:\n",
    "            sum = sum + int(x[11]) # 11th index (12th elem) is the quality\n",
    "        \n",
    "        g= float(float(sum)/float(len(data)))\n",
    "        return g\n",
    "    \n",
    "    def learn(self, training_set): #returns a node with the split conditions.\n",
    "        # implement this function\n",
    "         \n",
    "        if len(training_set) == 0:\n",
    "            return None\n",
    "        \n",
    "        tpl = self.splitNode(training_set)\n",
    "        root = Node(tpl[1], tpl[2], 2)\n",
    "        \n",
    "        #print(\"got tuple to split: @ attrib: %d, value: %f\" % (root.attrib, root.val) )\n",
    "        \n",
    "        if self.prob1(training_set) >= 0.95:\n",
    "            root.ans = 1\n",
    "            root.class1 = 1\n",
    "            return root\n",
    "        \n",
    "        elif self.prob1(training_set) <= 0.05:\n",
    "            root.ans = 0\n",
    "            root.class0 = 0\n",
    "            return root\n",
    "        \n",
    "        else:\n",
    "            l1 = [x for x in training_set if float(x[root.attrib]) <= root.val]\n",
    "            l2 = [x for x in training_set if float(x[root.attrib]) > root.val]\n",
    "            #print(\"left: %d, right %d\" %(len(l1), len(l2)))\n",
    "            root.left = self.learn(l1)\n",
    "            root.right = self.learn(l2)\n",
    "            \n",
    "            #for pruning purposes. \n",
    "            if not root.left is None:\n",
    "                root.class1 += root.left.class1\n",
    "                root.class0 += root.left.class0\n",
    "            \n",
    "            if not root.right is None:\n",
    "                root.class1 += root.right.class1\n",
    "                root.class0 += root.right.class0\n",
    "            \n",
    "            return root\n",
    "\n",
    "    # implement this function\n",
    "    def classify(self, test_instance):\n",
    "        #result = 0 # baseline: always classifies as 0\n",
    "        result = self.treeTraverse(test_instance, self.tree)\n",
    "        return result\n",
    "    \n",
    "    def treeTraverse(self, x_vector, node: Node) -> int:\n",
    "        if node is None:\n",
    "            return -1\n",
    "        elif not node.isLeaf():\n",
    "            return node.ans\n",
    "        elif float(x_vector[node.attrib]) <= node.val:\n",
    "            return self.treeTraverse(x_vector, node.left)\n",
    "        else:\n",
    "            return self.treeTraverse(x_vector, node.right)\n",
    "            \n",
    "        \n",
    "    \n",
    "    def splitNode(self, data): # returns the minimum entropy.\n",
    "        me = float(11000)\n",
    "        index = 0\n",
    "        Nm = len(data)\n",
    "        mSVal = 0.0\n",
    "        \n",
    "        for i in range(11): # 11 is the number of attributes \n",
    "            sv = self.splitVal(data, i)\n",
    "            l1 = [x for x in data if float(x[i]) <= sv]\n",
    "            l2 = [x for x in data if float(x[i]) > sv]\n",
    "            pa1 = self.prob1(l1)\n",
    "            pa2 = self.prob1(l2)\n",
    "            \n",
    "            eThis = self.entropy(pa1, len(l1), Nm) + self.entropy(pa2, len(l2), Nm)\n",
    "            #print(\"Entropy got: %f, me was: \" % eThis, me)\n",
    "            #print(eThis < me)\n",
    "                \n",
    "            if eThis < me:\n",
    "                me = eThis\n",
    "                index = i\n",
    "                mSVal = sv\n",
    "            '''\n",
    "            me = eThis if eThis < me else me\n",
    "            index = i if eThis < me else index\n",
    "            print(\"inedx now is: %d\" % index)\n",
    "            mSVal = sv if eThis < me else mSVal\n",
    "            '''\n",
    "            \n",
    "            #min = eThis < min ? eThis : min\n",
    "        return (me, index, mSVal)\n",
    "    \n",
    "    def pruneTree(self, maxChild: int, pruneSet) -> bool:\n",
    "        node = self.tree\n",
    "        \n",
    "        self.pruneHelper(maxChild, node)\n",
    "        \n",
    "        results = []\n",
    "        for instance in pruneSet:\n",
    "            result = self.classify( instance[:-1] )\n",
    "            results.append( (result) == int(instance[-1]))\n",
    "            #print (\"Actual: %d, classified as: %d\" %(int(instance[-1]), int(result)))\n",
    "        \n",
    "        # Accuracy\n",
    "        accuracy = float(results.count(True))/float(len(results))\n",
    "        print (\"prune: accuracy: %.4f\" % accuracy)       \n",
    "        \n",
    "                \n",
    "    def pruneHelper(self, maxChild, node):\n",
    "        s = node.class1 + node.class0\n",
    "        \n",
    "        if s<=maxChild:\n",
    "            node.ans = 1 if node.class1 >= node.class0 else 0\n",
    "        \n",
    "        else:\n",
    "            self.pruneHelper(maxChild, node.left)\n",
    "            self.pruneHelper(maxChild, node.right)\n",
    "            \n",
    "    \n",
    "def run_decision_tree():\n",
    "\n",
    "    # Load data set\n",
    "    with open(\"wine-dataset.csv\") as f:\n",
    "        next(f, None)\n",
    "        data = [tuple(line) for line in csv.reader(f, delimiter=\",\")]\n",
    "    print (\"Number of records: %d\" % (len(data)))\n",
    "\n",
    "    # Split training/test sets\n",
    "    # You need to modify the following code for cross validation.\n",
    "    K = 10\n",
    "    training_set = [x for i, x in enumerate(data) if i % K != 2 ]\n",
    "    test_set = [x for i, x in enumerate(data) if i % K == 2]\n",
    "    \n",
    "    \n",
    "    tree = DecisionTree()\n",
    "    # Construct a tree using training set\n",
    "    tree.tree = tree.learn( training_set )\n",
    "\n",
    "    \n",
    "    # Classify the test set using the tree we just constructed\n",
    "    results = []\n",
    "    for instance in test_set:\n",
    "        result = tree.classify( instance[:-1] )\n",
    "        results.append( (result) == int(instance[-1]))\n",
    "        #print (\"Actual: %d, classified as: %d\" %(int(instance[-1]), int(result)))\n",
    "\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = float(results.count(True))/float(len(results))\n",
    "    print (\"accuracy: %.4f\" % accuracy)       \n",
    "    \n",
    "\n",
    "    # Writing results to a file (DO NOT CHANGE)\n",
    "    f = open(myname+\"result.txt\", \"w\")\n",
    "    f.write(\"accuracy: %.4f\" % accuracy)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_decision_tree()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For prepruning, I have made leaf the nodes which had low entropy (probability of either class >= 0.95) - those nodes classify data points corresponding to the classes which they represent. \n",
    "\n",
    "This resulted in a slight boost in accuracy: no pruning accuracy was 0.8204 and accuracy after pruning was 0.8245.\n",
    "\n",
    "This is because pruning decreases over-fitting the training data and allows for more variation in test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
